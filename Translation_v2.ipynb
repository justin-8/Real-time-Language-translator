{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48c4f9-6c1d-46ad-8ff3-899ba3c53aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "import spacy\n",
    "import random\n",
    "import speech_recognition as sr\n",
    "import cv2\n",
    "import pytesseract\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from sacrebleu import corpus_bleu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0cd5f-acd3-4bb6-b779-243248bdd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load language models for tokenization\n",
    "spacy_german = spacy.load('de_core_news_sm')\n",
    "spacy_english = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_german(text):\n",
    "    \"\"\"Tokenize German text.\"\"\"\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    \"\"\"Tokenize English text.\"\"\"\n",
    "    return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "# Set up fields for text data\n",
    "SOURCE = Field(tokenize=tokenize_german, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TARGET = Field(tokenize=tokenize_english, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Load and split the dataset\n",
    "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SOURCE, TARGET))\n",
    "\n",
    "# Build vocabularies\n",
    "SOURCE.build_vocab(train_data, min_freq=2)\n",
    "TARGET.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Create data iterators\n",
    "BATCH_SIZE = 32\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d2409-4da6-41d8-96e2-0b6ebabf2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes the input sequence.\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        _, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decodes the encoded sequence.\"\"\"\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2SeqTranslator(nn.Module):\n",
    "    \"\"\"Sequence-to-sequence model for translation.\"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec891c6-9d8e-49ca-b14e-47feef2835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up model parameters\n",
    "INPUT_DIM = len(SOURCE.vocab)\n",
    "OUTPUT_DIM = len(TARGET.vocab)\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Initialize the model\n",
    "encoder = Encoder(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "model = Seq2SeqTranslator(encoder, decoder, device).to(device)\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi[TARGET.pad_token])\n",
    "\n",
    "# Evaluation metrics\n",
    "def calculate_bleu(model, iterator, target_vocab, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            output = output.argmax(dim=-1)\n",
    "            \n",
    "            for j in range(output.shape[1]):\n",
    "                trg_tokens = [target_vocab.itos[token] for token in trg[1:, j]]\n",
    "                pred_tokens = [target_vocab.itos[token] for token in output[1:, j]]\n",
    "                \n",
    "                # Remove <eos> token\n",
    "                trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]\n",
    "                pred_tokens = pred_tokens[:pred_tokens.index('<eos>') if '<eos>' in pred_tokens else None]\n",
    "                \n",
    "                references.append([trg_tokens])\n",
    "                predictions.append(pred_tokens)\n",
    "\n",
    "    return corpus_bleu(predictions, references)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training function (updated with progress bar)\n",
    "def train_model(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Prediction function for the custom model\n",
    "def predict_translation(model, sentence, source_vocab, target_vocab, device, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = [token.lower() for token in tokenize_german(sentence)]\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "    src_indexes = [source_vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [target_vocab.stoi['<sos>']]\n",
    "\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, _ = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == target_vocab.stoi['<eos>']:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [target_vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:-1]  # Remove <sos> and <eos>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9429597-0282-4b38-8fdc-6c8aada80c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5023d-6061-4342-8f3c-77ea3898cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio translation setup\n",
    "speech_recognizer = sr.Recognizer()\n",
    "\n",
    "def translate_audio(audio_file, src_lang='en', tgt_lang='de'):\n",
    "    \"\"\"Translate audio file.\"\"\"\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = speech_recognizer.record(source)\n",
    "    try:\n",
    "        text = speech_recognizer.recognize_google(audio, language=src_lang)\n",
    "        return translate_text(text, src_lang, tgt_lang)\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Speech recognition could not understand the audio\"\n",
    "    except sr.RequestError:\n",
    "        return \"Could not request results from speech recognition service\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa942203-3aa6-4849-a831-3ea6d07c139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video translation setup\n",
    "def translate_video(video_file, src_lang='en', tgt_lang='de'):\n",
    "    \"\"\"Extract and translate text from video frames.\"\"\"\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    translations = []\n",
    "    \n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        text = pytesseract.image_to_string(frame)\n",
    "        if text.strip():\n",
    "            translated = translate_text(text, src_lang, tgt_lang)\n",
    "            timestamp = video.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "            translations.append((timestamp, translated))\n",
    "    \n",
    "    video.release()\n",
    "    return translations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5babb-2127-40b1-b1ab-12fab64dc9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text translation using pre-trained model\n",
    "marian_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "marian_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "def translate_text(text, src_lang='en', tgt_lang='de'):\n",
    "    inputs = marian_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = marian_model.generate(**inputs)\n",
    "    return marian_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521c3b9-c2cd-49ed-8b0d-cb66f75d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the custom model (uncomment to train)\n",
    "    # N_EPOCHS = 10\n",
    "    # CLIP = 1\n",
    "    # for epoch in range(N_EPOCHS):\n",
    "    #     train_loss = train_model(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    #     print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n",
    "\n",
    "    # Example usage\n",
    "    print(\"Text Translation:\")\n",
    "    print(translate_text(\"Hello, how are you?\", 'en', 'de'))\n",
    "\n",
    "    print(\"\\nAudio Translation:\")\n",
    "    print(translate_audio(\"path_to_audio_file.wav\", 'en', 'de'))\n",
    "\n",
    "    print(\"\\nVideo Translation:\")\n",
    "    video_translations = translate_video(\"path_to_video_file.mp4\", 'en', 'de')\n",
    "    for timestamp, translation in video_translations:\n",
    "        print(f\"At {timestamp:.2f}s: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb94e9-5ecc-484e-88dd-2cfc4a8dc60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
